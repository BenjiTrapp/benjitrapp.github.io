---
layout: defense
title: DFIR - Digital Forensic Incident Response
---

<img height="120" align="left" src="/images/dfir-logo.jpeg" >
Digital Forensics and Incident Response (DFIR) is a field within cybersecurity that focuses on the identification, investigation, and remediation of cyber attacks.

DFIR has two main components:
* __Digital Forensic__: A subset of forensic science that examines system data, user activity, and other pieces of digital evidence to determine if an attack is in progress and who may be behind the activity.
* __Incident Response__: The overarching process that an organization will follow in order to prepare for, detect, contain, and recover from a data breach.


## Best resources to dive into DFIR

* [thedfirreport.com](https://thedfirreport.com/) - Learn from attacks by reading DFIR Reports and adopt IOCs 
* [detection.fyi](https://detection.fyi/) - Awesome resource for tons of Sigma Rules (thedfirreport also has great Sigma Rules) 
* [Sigma-CLI](https://github.com/SigmaHQ/sigma-cli) - Starting point for learning Sigma Rules
* [Tsurugi-DFIR-Linux](https://tsurugi-linux.org/)
* [FalconHound](https://github.com/FalconForceTeam/FalconHound) - FalconHound is a blue team multi-tool. It allows you to utilize and enhance the power of BloodHound in a more automated fashion.


## Sigma Rules

#### What are Sigma Rules
[Sigma](https://sigmahq.io/) is a generic signature format that allows to make detections in log events. Rules are easy to write and applicable to any type of log. Best of all, Sigma rules are abstract and not tied to any particular SIEM. This helps in making Sigma rules shareable.

Once a Cyber Security Researcher, Defender or Analyst develops a detection method, they can use Sigma to describe and share their technique with others. Here’s a quote from Sigma HQ:

> Sigma is for log files what Snort is for network traffic and YARA is for files.

Sigma can be seen on a high level like this:

![](images/cyberchef/Sigma_description_dark.png)

Let’s look at an example from Sigma HQ:

![](/images/sigma_example.png)


The heart of aboves rule is the detection section. When the condition evaluates to true it means the rule made a detection. A condition is composed of named expressions. For example, here a selection and a filter expression are declared. These expressions perform a tests against attributes of the logs (in this particular case web logs).

#### Anatomy of a Sigma Rule
![](/images/sigma_rule.jpeg)

Source: [Twitter - fr0gger](https://twitter.com/fr0gger_/status/1417517811442798593?lang=en)

#### Sigmac Generates SQL
The sigmac compiler is used to translate an abstract Sigma rule into a concrete form which can be evaluated by an actual SIEM or processing platform. Sigmac has many backends capable of translating a rule into an arbitrary SIEM like: Splunk, QRadar, ElasticSearch, ArcSight, FortiSIEM and generic SQL.

Using the SQL sigmac backend we can translate the rule above into:

```sql
SELECT 
    * 
FROM
    (
    SELECT
    (
        (cs-uri-query LIKE '%cmd=read%'
        OR cs-uri-query LIKE '%connect&target%'
        OR cs-uri-query LIKE '%cmd=connect%'
        OR cs-uri-query LIKE '%cmd=disconnect%'
        OR cs-uri-query LIKE '%cmd=forward%')
        AND (cs-referer IS NULL
        AND cs-USER-agent IS NULL
        AND cs-METHOD LIKE 'POST')) 
 AS web_webshell_regeorg,
       *
   FROM
    test_webserver_logs
   )
WHERE 
     web_webshell_regeorg = TRUE
```
These SQL statements are typically invoked by a scheduler on particular trigger interval; say 1 hour. Every hour the detection searches through the newest events. Such a scheduler could also be enhanced using Spark or other toolings.

However, some Sigma rules apply temporal aggregations. For example [Enumeration via the Global Catalog](https://github.com/SigmaHQ/sigma/blob/7c36a33ea77e94cbb5ad58fa061a84ed74dd503a/rules/windows/builtin/security/win_global_catalog_enumeration.yml) counts occurrence of events in a window of time.

```yaml
detection:
    selection:
        EventID: 5156
        DestPort:
        - 3268
        - 3269
    timeframe: 1h
    condition: selection | count() by SourceAddress > 2000
```

Using the batch model above, these types of queries reprocess the same events over and over. Especially if the correlation window is large.Therefore it might be a good idea to reduce the detection latency by increasing the trigger rate to a shorter window like every 5 minutes. Doing so this will introduce even more reprocessing of the same events.

Ideally to reduce reprocessing of the same events over and over again, something like anomaly detection can help to remember what was the last event it processed and what were the value of the counters so. That is exactly what Spark Structured Streaming framework was built for. A streaming query triggers a micro-batch every minute (configurable). It reads in the new events, updates all counters and persists them (for disaster recovery).

Spark Structured streaming can easily evaluate the SQL produced by the sigmac compiler. FTo achieve this it is required to create a streaming data frame by connecting to a queuing mechanism (EventHubs, Pub/Sub, Kafka, ...).

## Benji's Linux DFIR Script

This Bash script is designed for conducting digital forensics on Linux systems. The script automates the collection of a wide range of system and user data that could be used during DFIR investigations.

__Features:__

- **System Information**: Collects basic system information including uptime, startup time, and hardware clock readouts.
- **Operating System Details**: Extract information about the operating system installation, including installer logs and file system details.
- **Network Information**: Gathers network configuration, IP addresses, and network interface details.
- **Installed Programs**: Lists all installed packages using both `rpm` and `apt`.
- **Hardware Information**: Retrieves detailed information about PCI devices, hardware summaries, and BIOS data.
- **System Logs**: Captures system journal logs and the contents of the `/var/log` directory.
- **User Data**: Extracts user-specific data like recently used files and bash command history and zsh command history.
- **Memory Dump**: Performs a memory dump for detailed analysis.
- **Process Information**: Captures information about current running processes.
- **User Login History**: Records user login history and scheduled tasks.
- **Secure Output Handling**: Compresses and encrypts the gathered data for security.

 Usage:

 **Set Permissions**: Ensure the script is executable:
   ```bash
   sudo apt-get install util-linux  # For Debian/Ubuntu
   sudo yum install util-linux  # For CentOS/RHEL
   chmod +x DFLinux.sh
   sudo ./DFLinux.sh
   ```
   Output: Check the specified output directory for the collected data.

Requirements:
- The script is intended for use on Linux systems.
- Please make sure you have the necessary permissions to execute the script and access system files.
- Required tools: dump, gpg, netstat, ifconfig, lshw, dmidecode, etc., should be installed.

<!-- cSpell:disable -->
```bash
#!/bin/bash

output_dir="/tmp/ExtractedInfo"
logfile="$output_dir/forensics_log.txt"

initialize_logs() {
    echo "Forensic data extraction started at $(date)" > "$logfile"
}

write_output() {
    local command="$1"
    local filename="$2"

    if $command > "$output_dir/$filename" 2>&1; then
        echo "Successfully executed: $command" >> "$logfile"
    else
        echo "Failed to execute: $command" >> "$logfile"
    fi
}

check_and_write_history() {
    local user_home="$1"
    local username="$2"

    if [ -f "$user_home/.bash_history" ]; then
        write_output "cat $user_home/.bash_history" "bash_command_history_$username.txt"
    else
        echo "No .bash_history for $username" >> "$output_dir/bash_command_history_$username.txt"
    fi

    if [ -f "$user_home/.zsh_history" ]; then
        write_output "cat $user_home/.zsh_history" "zsh_command_history_$username.txt"
    else
        echo "No .zsh_history for $username" >> "$output_dir/zsh_command_history_$username.txt"
    fi

    write_output "cat $user_home/.local/share/recently-used.xbel" "recently_used_files_$username.txt"
}

initialize_logs

# System Information Extraction
write_output "uptime -p" "system_uptime.txt"
write_output "uptime -s" "system_startup_time.txt"
write_output "date" "current_system_date.txt"
write_output "date +%s" "current_unix_timestamp.txt"

if command -v hwclock &>/dev/null; then
    write_output "hwclock -r" "hardware_clock_readout.txt"
else
    echo "hwclock command not found" >> "$logfile"
fi

# Operating System Installation Date
write_output "df -P /" "root_filesystem_info.txt"
write_output "ls -l /var/log/installer" "os_installer_log.txt"
write_output "tune2fs -l /dev/sda1" "root_partition_filesystem_details.txt" # Check for correct root partition

# Network Information
write_output "ifconfig" "network_configuration.txt"
write_output "ip addr" "ip_address_info.txt"
write_output "netstat -i" "network_interfaces.txt"

# Installed Programs
write_output "dpkg -l" "dpkg_installed_packages.txt" # Replaced 'apt' with 'dpkg -l'
write_output "rpm -qa" "rpm_installed_packages.txt"

# Hardware Information
write_output "lspci" "pci_device_list.txt"
write_output "lshw -short" "hardware_summary_report.txt"
write_output "dmidecode" "dmi_bios_info.txt"

# System Logs and Usage
write_output "journalctl" "system_journal_logs.txt"
write_output "ls -lah /var/log/" "var_log_directory_listing.txt"

for user_home in /home/*; do
    username=$(basename "$user_home")
    check_and_write_history "$user_home" "$username"
done

if crontab -l &>/dev/null; then
    write_output "crontab -l" "scheduled_cron_jobs_root.txt"
else
    echo "No crontab for root" >> "$output_dir/scheduled_cron_jobs_root.txt"
fi

tar -czf "$output_dir/user_data.tar.gz" -C "$output_dir" ./*.txt --remove-files

echo "Data extraction complete. Check the $output_dir directory for output." >> "$logfile"
echo "Forensic data extraction completed at $(date)" >> "$logfile"

echo "Data extraction complete. Check the $output_dir directory for output."
```
<!-- cSpell:enable -->